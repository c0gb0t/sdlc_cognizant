{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install deps\n",
    "# %pip install azure-cognitiveservices-speech\n",
    "# %pip install openai\n",
    "# # -----------------\n",
    "# # %pip install azure-identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "inc='inc1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ./data_dir/input_files/inc1\n",
      "./data_dir/input_files/inc1/jackhammer.wav\n",
      "./data_dir/input_files/inc1/mail2.txt\n",
      "./data_dir/input_files/inc1/harvard.wav\n",
      "./data_dir/input_files/inc1/mail1.txt\n"
     ]
    }
   ],
   "source": [
    "# read files form data_dir/input_files directory and create a list of filepaths\n",
    "def create_file_hm(inc):\n",
    "    import os\n",
    "    input_dir=f'./data_dir/input_files/{inc}'\n",
    "    print(f\"Files in the directory: {input_dir}\")\n",
    "    files = os.listdir(input_dir)\n",
    "    # Filtering only the files.\n",
    "    # files = [f for f in files if os.path.isfile(input_dir+'/'+f)]\n",
    "    files = [f for f in files]\n",
    "    files=[f'{input_dir}/{f}' for f in files]\n",
    "    print(*files, sep=\"\\n\")\n",
    "    # fd={}\n",
    "    # for f in files:\n",
    "    #     fd[f]=open(f'{input_dir}/{f}')\n",
    "    # print(fd)\n",
    "    return files\n",
    "files=create_file_hm(inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ./data_dir/input_files/inc1/mail2.txt file----------------\n",
      "reading ./data_dir/input_files/inc1/mail1.txt file----------------\n",
      "this mail contains summary about the incident 34567\n"
     ]
    }
   ],
   "source": [
    "# processing text files\n",
    "def proc_txt_data(files):\n",
    "    txt_files=[f for f in files if '.txt' in f]\n",
    "    txt_data=\"\"\n",
    "    for tf in txt_files:\n",
    "        print(f\"reading {tf} file----------------\")\n",
    "        with open(tf) as f:\n",
    "            txt_data+='\\n'.join(f.readlines())\n",
    "    print(txt_data)\n",
    "    return txt_data\n",
    "txt_data=proc_txt_data(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['stt_key1', 'stt_key2', 'openai_key1', 'openai_key2'])\n"
     ]
    }
   ],
   "source": [
    "# load azure keys\n",
    "import json\n",
    "with open('../keys/azure.json') as json_file:\n",
    "    azure_keys = json.load(json_file)\n",
    "print(azure_keys.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stale smell of old beer lingers.\n",
      "The stale smell of old beer lingers. It takes heat to bring out the odor. A cold dip restores health and zest. A salt pickle tastes fine with ham tacos. Al pastora are my favorite. A zestful food is the hot cross bun.\n"
     ]
    }
   ],
   "source": [
    "# extract data from azure speech to text api : convert speech to text\n",
    "import sys\n",
    "# %pip install moviepy\n",
    "# from moviepy.editor import *\n",
    "\n",
    "# %pip install azure==4.0.0\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "\n",
    "def proc_video_data(files):\n",
    "    aud_files=[f for f in files if '.mp4' in f or '.wav' in f]\n",
    "    \n",
    "    aud_data=\"\"\n",
    "    for af in aud_files:\n",
    "        speech_key, service_region = azure_keys['stt_key1'], \"eastus\"\n",
    "        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "        audio_config = speechsdk.audio.AudioConfig(filename=af)\n",
    "        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "        result = speech_recognizer.recognize_once()\n",
    "        print(result.text)\n",
    "        aud_data+=\"\\n\"+result.text\n",
    "    return aud_data\n",
    "\n",
    "aud_data=proc_video_data(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
    "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
    "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
    "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
    "\"\"\"\n",
    "\n",
    "# content=txt_data+aud_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content to summarize----------------\n",
      " \n",
      "At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "\n",
      "\n",
      "summary--------------------------\n",
      " \n",
      "Key Action Items:\n",
      "1. Develop a joint XYZ-code representation to enhance AI capabilities.\n",
      "2. Conduct further research and testing to achieve cross-domain transfer learning.\n",
      "3. Incorporate external knowledge sources in downstream AI tasks.\n",
      "\n",
      "Decisions Made:\n",
      "1. The team will focus on developing the joint XYZ-code representation as a foundational component for achieving multi-sensory and multilingual learning in AI.\n",
      "2. Further research and testing will be conducted to achieve cross-domain transfer learning.\n",
      "3. External knowledge sources will be incorporated in downstream AI tasks.\n",
      "\n",
      "Deadlines Set:\n",
      "1. Develop and test the joint XYZ-code representation within the next 6 months.\n",
      "2. Conduct research and testing for cross-domain transfer learning within the next year.\n",
      "3. Incorporate external knowledge sources in downstream AI tasks within the next 3 months.\n"
     ]
    }
   ],
   "source": [
    "# create summary from azure open ai api\n",
    "def generate_summary(content):\n",
    "    print(\"content to summarize----------------\\n\",content)\n",
    "    prompt=f\"\"\" \n",
    "    Generate a summary of our team meeting, highlighting key action items, decisions made, and deadlines set in key points. \n",
    "    #meeting transcript start:\n",
    "    {content}\n",
    "    #meeting transcript end\n",
    "    \"\"\"\n",
    "    from openai import AzureOpenAI\n",
    "    # from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "    deployment = \"red-gpt\"\n",
    "    endpoint=\"https://genai-warriors-openai-test.openai.azure.com/\"\n",
    "    # token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        # azure_ad_token_provider=token_provider,\n",
    "        api_version=\"2024-02-01\",\n",
    "        api_key=azure_keys['openai_key1']\n",
    "    )\n",
    "    completion = client.completions.create(\n",
    "        model=deployment,\n",
    "        prompt=prompt,\n",
    "        temperature=0.3,\n",
    "        max_tokens=350,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    response=completion\n",
    "    res=response.choices[0].text\n",
    "\n",
    "\n",
    "    print(\"\\n\\nsummary--------------------------\\n\",res)\n",
    "    return res\n",
    "   \n",
    "\n",
    "summary=generate_summary(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
