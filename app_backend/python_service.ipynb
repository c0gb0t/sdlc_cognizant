{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.content_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['stt_key1', 'stt_key2', 'openai_key1', 'openai_key2'])\n"
     ]
    }
   ],
   "source": [
    "# load azure keys\n",
    "import json\n",
    "with open('../keys/azure.json') as json_file:\n",
    "    azure_keys = json.load(json_file)\n",
    "print(azure_keys.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch files from url\n",
    "import requests\n",
    "\n",
    "def load_files_from_url(fileUrls):\n",
    "    urls=fileUrls\n",
    "    files=[]\n",
    "    for url in urls:\n",
    "        print(\"url------------------------\\n\",url)\n",
    "        local_filename = url.split('/')[-1].split(\"%2F\")[1].split(\"?\")[0]\n",
    "        local_filename='./data_dir/input_files/'+local_filename\n",
    "        # NOTE the stream=True parameter below\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(local_filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        file_path=local_filename\n",
    "        files.append(file_path)\n",
    "    print(\"recieved file paths---------------------------\\n\",files)\n",
    "    return files\n",
    "\n",
    "# load_files_from_url([' https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Ffiles%2Fmail1.txt?alt=media&token=840a21e2-a35c-481f-9ebb-677854630d02'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing text files\n",
    "def proc_txt_data(files):\n",
    "    txt_files=[f for f in files if '.txt' in f]\n",
    "    txt_data=\"\"\n",
    "    for tf in txt_files:\n",
    "        print(f\"reading {tf} file----------------\")\n",
    "        with open(tf) as f:\n",
    "            txt_data+='\\n'.join(f.readlines())\n",
    "    print(\"processed text data------------------------\\n\",txt_data)\n",
    "    return txt_data\n",
    "\n",
    "# txt_data=proc_txt_data(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from azure speech to text api : convert speech to text\n",
    "import sys\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def proc_video_data(files):\n",
    "    aud_files=[f for f in files if '.mp4' in f or '.wav' in f]\n",
    "    \n",
    "    aud_data=\"\"\n",
    "    for af in aud_files:\n",
    "        speech_key, service_region = azure_keys['stt_key1'], \"eastus\"\n",
    "        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "        audio_config = speechsdk.audio.AudioConfig(filename=af)\n",
    "        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "        result = speech_recognizer.recognize_once()\n",
    "        print(result.text)\n",
    "        aud_data+=\"\\n\"+result.text\n",
    "    return aud_data\n",
    "\n",
    "# aud_data=proc_video_data(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
    "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
    "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
    "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
    "\"\"\"\n",
    "\n",
    "# content=txt_data+aud_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary from azure open ai api\n",
    "def generate_summary(content):\n",
    "    \n",
    "    print(\"content to summarize----------------------\\n\",content.__str__())\n",
    "    # prompt=f\"\"\" \n",
    "    # Generate a summary of our team meeting, highlighting key action items, decisions made, and deadlines set in key points. \n",
    "    # #meeting transcript start:\n",
    "    # {content}\n",
    "    # #meeting transcript end\n",
    "    # \"\"\"\n",
    "\n",
    "    prompt=f\"\"\" \n",
    "    Generate a summary of our team meeting, highlighting in key points:\n",
    "    1)Introduction\n",
    "    2)Requirements Overview\n",
    "    3) In scope\n",
    "    4) out of scope\n",
    "    5) assumptions\n",
    "    6) Notes\n",
    "     \n",
    "    #meeting transcript start:\n",
    "    {content}\n",
    "    #meeting transcript end\n",
    "    \"\"\"\n",
    "    from openai import AzureOpenAI\n",
    "    # from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "    deployment = \"red-gpt\"\n",
    "    endpoint=\"https://genai-warriors-openai-test.openai.azure.com/\"\n",
    "    # token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        # azure_ad_token_provider=token_provider,\n",
    "        api_version=\"2024-02-01\",\n",
    "        api_key=azure_keys['openai_key1']\n",
    "    )\n",
    "    completion = client.completions.create(\n",
    "        model=deployment,\n",
    "        prompt=prompt,\n",
    "        temperature=0.3,\n",
    "        max_tokens=350,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    response=completion\n",
    "    print(response)\n",
    "    cfr=response.choices[0].content_filter_results\n",
    "    pfr=response.prompt_filter_results\n",
    "    res=response.choices[0].text\n",
    "\n",
    "\n",
    "    print(\"\\n\\nsummary--------------------------\\n\",res)\n",
    "    print(cfr,'\\n',pfr)\n",
    "    # return response\n",
    "    return res +'\\n\\nContent Filter results=\\n'+pfr.__str__()\n",
    "   \n",
    "\n",
    "# response=generate_summary(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listener function\n",
    "from models.content_model import ContentModel\n",
    "# %pip install firebase_admin\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "import datetime\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from pandas import Timestamp\n",
    "\n",
    "\n",
    "\n",
    "def listen_msgs():\n",
    "    coll_name='sdlc_cognizant'\n",
    "    user_uid='sdlc_cognizant_user'\n",
    "        \n",
    "    # Use a service account.\n",
    "    cred = credentials.Certificate(f'../keys/sa.json')\n",
    "    if not firebase_admin._apps:\n",
    "        app = firebase_admin.initialize_app(cred)\n",
    "\n",
    "    db = firestore.client()\n",
    "    \n",
    "    # Create an Event for notifying main thread.\n",
    "    callback_done = threading.Event()\n",
    "\n",
    "    # Create a callback on_snapshot function to capture changes\n",
    "    def on_snapshot(doc_snapshot, changes, read_time):\n",
    "        for doc in doc_snapshot:\n",
    "            # print(\"doc-------------------------------\\n\",doc_snapshot)\n",
    "            print(f\"Received document snapshot: {doc.id}\")\n",
    "            print(doc.to_dict())\n",
    "            content=ContentModel.from_dict(doc.to_dict())\n",
    "            # complaint.complaintText=complaint.complaintText.toString()\n",
    "            # data=doc.to_dict()\n",
    "            # if content.audioURL!='':\n",
    "            #     content.complaintText=convert_stt(complaint=content)\n",
    "            # if content.lang!='english':\n",
    "            #     content.complaintText=translate(complaint=content)\n",
    "\n",
    "            files=load_files_from_url(content.fileUrls)\n",
    "            txt_data=proc_txt_data(files=files)\n",
    "            vid_data=proc_video_data(files=files)\n",
    "            summary=generate_summary(txt_data+vid_data)\n",
    "            # pred_res={'output':\"model response\"}\n",
    "\n",
    "            content.senderId='backend@red'\n",
    "            content.output=summary\n",
    "            # data['priority']=pred_res['output'].split(':')[-1]\n",
    "            content.ots=datetime.datetime.utcnow()\n",
    "            print(f'sending response: '+content.output)\n",
    "            # doc_id=str(round(time.time() * 1000))\n",
    "            doc_id=content.id\n",
    "            db.collection(coll_name).document(user_uid).collection('pContents').document(doc_id).set(content.__dict__)\n",
    "            print(f\"----------sent----------------with id: {doc_id} \")\n",
    "\n",
    "        callback_done.set()\n",
    "\n",
    "    doc_ref = db.collection(coll_name).document(user_uid).collection('rContents').document(\"content\")\n",
    "\n",
    "    # Watch the document\n",
    "    doc_watch = doc_ref.on_snapshot(on_snapshot)\n",
    "    print(\"listening for messages...\",)\n",
    "    \n",
    "    \n",
    "\n",
    "    while True:\n",
    "        print('', end='', flush=True)\n",
    "        time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listening for messages...\n",
      "Received document snapshot: content\n",
      "{'ots': None, 'senderId': 'user@red', 'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=81f65ed5-22bd-4343-a833-842cabe42188'], 'id': '1715608659838', 'output': '', 'timestamp': DatetimeWithNanoseconds(2024, 5, 13, 13, 57, 39, 838000, tzinfo=datetime.timezone.utc)}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=81f65ed5-22bd-4343-a833-842cabe42188\n",
      "recieved file paths---------------------------\n",
      " ['./data_dir/input_files/mail1.txt']\n",
      "reading ./data_dir/input_files/mail1.txt file----------------\n",
      "processed text data------------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "content to summarize----------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Completion(id='cmpl-9OdDlRmRCrL4QVjjTaKvVY7Gh8hHh', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=\"\\n1) Introduction: The meeting began with the Chief Technology Officer of Azure AI Services discussing the team's quest to advance AI beyond existing techniques and their unique perspective on human cognition.\\n2) Requirements Overview: The team aims to create a joint representation, called XYZ-code, that combines monolingual text, audio or visual sensory signals, and multilingual data to create more powerful AI.\\n3) In Scope: The team has achieved human-level performance in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\\n4) Out of Scope: The team's ultimate goal is to achieve multi-sensory and multilingual learning that is closer to how humans learn and understand, which will require incorporating external knowledge sources.\\n5) Assumptions: The team believes that the joint XYZ-code is a foundational component for achieving their ambitious aspiration.\\n6) Notes: The team has been working with a team of scientists and engineers and has made significant progress in the past five years. They believe that their approach will lead to a leap in AI capabilities. \", content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1715657665, model='gpt-35-turbo-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=218, prompt_tokens=378, total_tokens=596), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "\n",
      "\n",
      "summary--------------------------\n",
      " \n",
      "1) Introduction: The meeting began with the Chief Technology Officer of Azure AI Services discussing the team's quest to advance AI beyond existing techniques and their unique perspective on human cognition.\n",
      "2) Requirements Overview: The team aims to create a joint representation, called XYZ-code, that combines monolingual text, audio or visual sensory signals, and multilingual data to create more powerful AI.\n",
      "3) In Scope: The team has achieved human-level performance in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n",
      "4) Out of Scope: The team's ultimate goal is to achieve multi-sensory and multilingual learning that is closer to how humans learn and understand, which will require incorporating external knowledge sources.\n",
      "5) Assumptions: The team believes that the joint XYZ-code is a foundational component for achieving their ambitious aspiration.\n",
      "6) Notes: The team has been working with a team of scientists and engineers and has made significant progress in the past five years. They believe that their approach will lead to a leap in AI capabilities. \n",
      "{'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}} \n",
      " [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "sending response: \n",
      "1) Introduction: The meeting began with the Chief Technology Officer of Azure AI Services discussing the team's quest to advance AI beyond existing techniques and their unique perspective on human cognition.\n",
      "2) Requirements Overview: The team aims to create a joint representation, called XYZ-code, that combines monolingual text, audio or visual sensory signals, and multilingual data to create more powerful AI.\n",
      "3) In Scope: The team has achieved human-level performance in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n",
      "4) Out of Scope: The team's ultimate goal is to achieve multi-sensory and multilingual learning that is closer to how humans learn and understand, which will require incorporating external knowledge sources.\n",
      "5) Assumptions: The team believes that the joint XYZ-code is a foundational component for achieving their ambitious aspiration.\n",
      "6) Notes: The team has been working with a team of scientists and engineers and has made significant progress in the past five years. They believe that their approach will lead to a leap in AI capabilities. \n",
      "\n",
      "Content Filter results=\n",
      "[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "----------sent----------------with id: 1715608659838 \n",
      "Received document snapshot: content\n",
      "{'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=f20a5051-58ff-41d1-b083-2c4d864dd150'], 'senderId': 'user@red', 'ots': None, 'id': '1715657852403', 'timestamp': DatetimeWithNanoseconds(2024, 5, 14, 3, 37, 32, 404000, tzinfo=datetime.timezone.utc), 'output': ''}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=f20a5051-58ff-41d1-b083-2c4d864dd150\n",
      "doc-------------------------------\n",
      " [<google.cloud.firestore_v1.base_document.DocumentSnapshot object at 0x79b01722a410>]\n",
      "Received document snapshot: content\n",
      "{'ots': None, 'senderId': 'user@red', 'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=f20a5051-58ff-41d1-b083-2c4d864dd150'], 'id': '1715657852403', 'output': '', 'timestamp': DatetimeWithNanoseconds(2024, 5, 14, 3, 37, 32, 404000, tzinfo=datetime.timezone.utc)}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=f20a5051-58ff-41d1-b083-2c4d864dd150\n",
      "Received document snapshot: content\n",
      "{'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=f20a5051-58ff-41d1-b083-2c4d864dd150'], 'senderId': 'user@red', 'ots': None, 'id': '1715657852403', 'timestamp': DatetimeWithNanoseconds(2024, 5, 14, 3, 37, 32, 404000, tzinfo=datetime.timezone.utc), 'output': ''}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=f20a5051-58ff-41d1-b083-2c4d864dd150\n",
      "doc-------------------------------\n",
      " [<google.cloud.firestore_v1.base_document.DocumentSnapshot object at 0x79b017268d30>]\n",
      "Received document snapshot: content\n",
      "{'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=f20a5051-58ff-41d1-b083-2c4d864dd150'], 'senderId': 'user@red', 'ots': None, 'id': '1715657852403', 'timestamp': DatetimeWithNanoseconds(2024, 5, 14, 3, 37, 32, 404000, tzinfo=datetime.timezone.utc), 'output': ''}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=f20a5051-58ff-41d1-b083-2c4d864dd150\n",
      "recieved file paths---------------------------\n",
      " ['./data_dir/input_files/mail1.txt']\n",
      "reading ./data_dir/input_files/mail1.txt file----------------\n",
      "processed text data------------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "content to summarize----------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "recieved file paths---------------------------\n",
      " ['./data_dir/input_files/mail1.txt']\n",
      "reading ./data_dir/input_files/mail1.txt file----------------\n",
      "processed text data------------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "content to summarize----------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "recieved file paths---------------------------\n",
      " ['./data_dir/input_files/mail1.txt']\n",
      "reading ./data_dir/input_files/mail1.txt file----------------\n",
      "processed text data------------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "content to summarize----------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "recieved file paths---------------------------\n",
      " ['./data_dir/input_files/mail1.txt']\n",
      "reading ./data_dir/input_files/mail1.txt file----------------\n",
      "processed text data------------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "content to summarize----------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Completion(id='cmpl-9OdGqWYfSUMQtWHQASWg5fkRxMUhy', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=\"\\n1) Introduction: The meeting began with the Chief Technology Officer of Azure AI Services discussing their quest to advance AI and their unique perspective on human cognition.\\n2) Requirements Overview: The team has been working on creating a joint representation, called XYZ-code, that combines monolingual text, audio or visual sensory signals, and multilingual capabilities to create more powerful AI.\\n3) In scope: The team has achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\\n4) Out of scope: The team's long-term vision is to achieve cross-domain transfer learning and multi-sensory and multilingual learning, which is closer to how humans learn and understand.\\n5) Assumptions: The team believes that the joint XYZ-code, when combined with external knowledge sources, will be a foundational component in achieving their ambitious aspirations.\\n6) Notes: The team has made significant breakthroughs in the past five years and is confident in their ability to produce a leap in AI capabilities. They also discussed the importance of human-centric learning and understanding in their approach.\", content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1715657856, model='gpt-35-turbo-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=227, prompt_tokens=378, total_tokens=605), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "\n",
      "\n",
      "summary--------------------------\n",
      " \n",
      "1) Introduction: The meeting began with the Chief Technology Officer of Azure AI Services discussing their quest to advance AI and their unique perspective on human cognition.\n",
      "2) Requirements Overview: The team has been working on creating a joint representation, called XYZ-code, that combines monolingual text, audio or visual sensory signals, and multilingual capabilities to create more powerful AI.\n",
      "3) In scope: The team has achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n",
      "4) Out of scope: The team's long-term vision is to achieve cross-domain transfer learning and multi-sensory and multilingual learning, which is closer to how humans learn and understand.\n",
      "5) Assumptions: The team believes that the joint XYZ-code, when combined with external knowledge sources, will be a foundational component in achieving their ambitious aspirations.\n",
      "6) Notes: The team has made significant breakthroughs in the past five years and is confident in their ability to produce a leap in AI capabilities. They also discussed the importance of human-centric learning and understanding in their approach.\n",
      "{'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}} \n",
      " [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "sending response: \n",
      "1) Introduction: The meeting began with the Chief Technology Officer of Azure AI Services discussing their quest to advance AI and their unique perspective on human cognition.\n",
      "2) Requirements Overview: The team has been working on creating a joint representation, called XYZ-code, that combines monolingual text, audio or visual sensory signals, and multilingual capabilities to create more powerful AI.\n",
      "3) In scope: The team has achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n",
      "4) Out of scope: The team's long-term vision is to achieve cross-domain transfer learning and multi-sensory and multilingual learning, which is closer to how humans learn and understand.\n",
      "5) Assumptions: The team believes that the joint XYZ-code, when combined with external knowledge sources, will be a foundational component in achieving their ambitious aspirations.\n",
      "6) Notes: The team has made significant breakthroughs in the past five years and is confident in their ability to produce a leap in AI capabilities. They also discussed the importance of human-centric learning and understanding in their approach.\n",
      "\n",
      "Content Filter results=\n",
      "[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "Completion(id='cmpl-9OdGrAM73b8eNrUnXTl9z3VPYH0dc', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text='\\n1) The meeting began with an introduction from the Chief Technology Officer of Azure AI Services, who discussed their quest to advance AI through a more holistic, human-centric approach.\\n2) The team then provided an overview of the requirements for this project, with a focus on the intersection of monolingual text, audio/visual signals, and multilingual learning.\\n3) The goal is to create a joint representation, called XYZ-code, that will enable pre-trained models to support a broad range of downstream AI tasks.\\n4) The team has already achieved human-level performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\\n5) The team believes that the joint XYZ-code, when combined with external knowledge sources, will lead to a significant leap in AI capabilities that is closer to how humans learn and understand.\\n6) Notes were taken throughout the meeting to document progress and ideas for future development.', content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1715657857, model='gpt-35-turbo-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=192, prompt_tokens=378, total_tokens=570), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "\n",
      "\n",
      "summary--------------------------\n",
      " \n",
      "1) The meeting began with an introduction from the Chief Technology Officer of Azure AI Services, who discussed their quest to advance AI through a more holistic, human-centric approach.\n",
      "2) The team then provided an overview of the requirements for this project, with a focus on the intersection of monolingual text, audio/visual signals, and multilingual learning.\n",
      "3) The goal is to create a joint representation, called XYZ-code, that will enable pre-trained models to support a broad range of downstream AI tasks.\n",
      "4) The team has already achieved human-level performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n",
      "5) The team believes that the joint XYZ-code, when combined with external knowledge sources, will lead to a significant leap in AI capabilities that is closer to how humans learn and understand.\n",
      "6) Notes were taken throughout the meeting to document progress and ideas for future development.\n",
      "{'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}} \n",
      " [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "sending response: \n",
      "1) The meeting began with an introduction from the Chief Technology Officer of Azure AI Services, who discussed their quest to advance AI through a more holistic, human-centric approach.\n",
      "2) The team then provided an overview of the requirements for this project, with a focus on the intersection of monolingual text, audio/visual signals, and multilingual learning.\n",
      "3) The goal is to create a joint representation, called XYZ-code, that will enable pre-trained models to support a broad range of downstream AI tasks.\n",
      "4) The team has already achieved human-level performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n",
      "5) The team believes that the joint XYZ-code, when combined with external knowledge sources, will lead to a significant leap in AI capabilities that is closer to how humans learn and understand.\n",
      "6) Notes were taken throughout the meeting to document progress and ideas for future development.\n",
      "\n",
      "Content Filter results=\n",
      "[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "Completion(id='cmpl-9OdGqqPgEilXUrkfwT4y0X9injl5a', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=\"\\n1) Introduction: The meeting began with an introduction from the Chief Technology Officer of Azure AI Services, who discussed the team's quest to advance AI through a holistic, human-centric approach.\\n2) Requirements Overview: The team has been working on developing a joint representation, called XYZ-code, which combines monolingual text, audio or visual signals, and multilingual capabilities to create more powerful AI.\\n3) In Scope: The team has achieved human-level performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\\n4) Out of Scope: The team's long-term vision is to achieve cross-domain transfer learning, spanning modalities and languages, but this is still a work in progress.\\n5) Assumptions: The team believes that the joint XYZ-code, when combined with external knowledge sources, will be a foundational component in achieving their long-term vision.\\n6) Notes: The team has made significant progress in the past five years, but there is still work to be done in order to reach their ambitious goals. They are confident that their approach will lead to a leap in AI capabilities, making it more similar to how humans learn and understand. \", content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1715657856, model='gpt-35-turbo-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=245, prompt_tokens=378, total_tokens=623), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "\n",
      "\n",
      "summary--------------------------\n",
      " \n",
      "1) Introduction: The meeting began with an introduction from the Chief Technology Officer of Azure AI Services, who discussed the team's quest to advance AI through a holistic, human-centric approach.\n",
      "2) Requirements Overview: The team has been working on developing a joint representation, called XYZ-code, which combines monolingual text, audio or visual signals, and multilingual capabilities to create more powerful AI.\n",
      "3) In Scope: The team has achieved human-level performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n",
      "4) Out of Scope: The team's long-term vision is to achieve cross-domain transfer learning, spanning modalities and languages, but this is still a work in progress.\n",
      "5) Assumptions: The team believes that the joint XYZ-code, when combined with external knowledge sources, will be a foundational component in achieving their long-term vision.\n",
      "6) Notes: The team has made significant progress in the past five years, but there is still work to be done in order to reach their ambitious goals. They are confident that their approach will lead to a leap in AI capabilities, making it more similar to how humans learn and understand. \n",
      "{'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}} \n",
      " [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "sending response: \n",
      "1) Introduction: The meeting began with an introduction from the Chief Technology Officer of Azure AI Services, who discussed the team's quest to advance AI through a holistic, human-centric approach.\n",
      "2) Requirements Overview: The team has been working on developing a joint representation, called XYZ-code, which combines monolingual text, audio or visual signals, and multilingual capabilities to create more powerful AI.\n",
      "3) In Scope: The team has achieved human-level performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n",
      "4) Out of Scope: The team's long-term vision is to achieve cross-domain transfer learning, spanning modalities and languages, but this is still a work in progress.\n",
      "5) Assumptions: The team believes that the joint XYZ-code, when combined with external knowledge sources, will be a foundational component in achieving their long-term vision.\n",
      "6) Notes: The team has made significant progress in the past five years, but there is still work to be done in order to reach their ambitious goals. They are confident that their approach will lead to a leap in AI capabilities, making it more similar to how humans learn and understand. \n",
      "\n",
      "Content Filter results=\n",
      "[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "Completion(id='cmpl-9OdGrEEjEuo9swexdS4TXW0wmHBDq', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=\"\\n1) Introduction: The meeting began with the Chief Technology Officer of Azure AI Services discussing their quest to advance AI and the importance of a holistic, human-centric approach to learning and understanding.\\n2) Requirements Overview: The team has been working on creating a joint representation, called XYZ-code, which combines monolingual text, audio or visual sensory signals, and multilingual data to create more powerful AI.\\n3) In Scope: The team has achieved human-level performance in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning, which are all part of their goal to achieve multi-sensory and multilingual learning.\\n4) Out of Scope: The team's ultimate goal is to produce a leap in AI capabilities, but they acknowledge that this may require external knowledge sources and further research.\\n5) Assumptions: The team assumes that the joint XYZ-code will be a foundational component in achieving their ambitious aspiration.\\n6) Notes: The team has been working on this project for the past five years and has made significant breakthroughs, providing strong signals towards their ultimate goal. They believe that their approach is closer in line with how humans learn and understand.\", content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1715657857, model='gpt-35-turbo-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=241, prompt_tokens=378, total_tokens=619), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "\n",
      "\n",
      "summary--------------------------\n",
      " \n",
      "1) Introduction: The meeting began with the Chief Technology Officer of Azure AI Services discussing their quest to advance AI and the importance of a holistic, human-centric approach to learning and understanding.\n",
      "2) Requirements Overview: The team has been working on creating a joint representation, called XYZ-code, which combines monolingual text, audio or visual sensory signals, and multilingual data to create more powerful AI.\n",
      "3) In Scope: The team has achieved human-level performance in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning, which are all part of their goal to achieve multi-sensory and multilingual learning.\n",
      "4) Out of Scope: The team's ultimate goal is to produce a leap in AI capabilities, but they acknowledge that this may require external knowledge sources and further research.\n",
      "5) Assumptions: The team assumes that the joint XYZ-code will be a foundational component in achieving their ambitious aspiration.\n",
      "6) Notes: The team has been working on this project for the past five years and has made significant breakthroughs, providing strong signals towards their ultimate goal. They believe that their approach is closer in line with how humans learn and understand.\n",
      "{'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}} \n",
      " [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "sending response: \n",
      "1) Introduction: The meeting began with the Chief Technology Officer of Azure AI Services discussing their quest to advance AI and the importance of a holistic, human-centric approach to learning and understanding.\n",
      "2) Requirements Overview: The team has been working on creating a joint representation, called XYZ-code, which combines monolingual text, audio or visual sensory signals, and multilingual data to create more powerful AI.\n",
      "3) In Scope: The team has achieved human-level performance in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning, which are all part of their goal to achieve multi-sensory and multilingual learning.\n",
      "4) Out of Scope: The team's ultimate goal is to produce a leap in AI capabilities, but they acknowledge that this may require external knowledge sources and further research.\n",
      "5) Assumptions: The team assumes that the joint XYZ-code will be a foundational component in achieving their ambitious aspiration.\n",
      "6) Notes: The team has been working on this project for the past five years and has made significant breakthroughs, providing strong signals towards their ultimate goal. They believe that their approach is closer in line with how humans learn and understand.\n",
      "\n",
      "Content Filter results=\n",
      "[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "----------sent----------------with id: 1715657852403 \n",
      "----------sent----------------with id: 1715657852403 \n",
      "----------sent----------------with id: 1715657852403 \n",
      "----------sent----------------with id: 1715657852403 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# calling the listener function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mlisten_msgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror occured:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \n",
      "Cell \u001b[0;32mIn[35], line 72\u001b[0m, in \u001b[0;36mlisten_msgs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 72\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# calling the listener function\n",
    "try:\n",
    "    listen_msgs()\n",
    "except Exception as e:\n",
    "    print(f'error occured:\\n\\n\\n {e}')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
