{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received document snapshot: content\n",
      "{'ots': None, 'senderId': 'user@red', 'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=3d4ea0bb-52ec-472a-8c00-b12f830c0d23', ''], 'id': '1715595297145', 'timestamp': DatetimeWithNanoseconds(2024, 5, 13, 10, 14, 57, 145000, tzinfo=datetime.timezone.utc), 'output': ''}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=3d4ea0bb-52ec-472a-8c00-b12f830c0d23\n",
      "Received document snapshot: content\n",
      "{'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=3d4ea0bb-52ec-472a-8c00-b12f830c0d23', ''], 'senderId': 'user@red', 'ots': None, 'id': '1715595297145', 'output': '', 'timestamp': DatetimeWithNanoseconds(2024, 5, 13, 10, 14, 57, 145000, tzinfo=datetime.timezone.utc)}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=3d4ea0bb-52ec-472a-8c00-b12f830c0d23\n",
      "Received document snapshot: content\n",
      "{'ots': None, 'senderId': 'user@red', 'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=3d4ea0bb-52ec-472a-8c00-b12f830c0d23', ''], 'id': '1715595297145', 'output': '', 'timestamp': DatetimeWithNanoseconds(2024, 5, 13, 10, 14, 57, 145000, tzinfo=datetime.timezone.utc)}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=3d4ea0bb-52ec-472a-8c00-b12f830c0d23\n",
      "Received document snapshot: content\n",
      "{'ots': None, 'senderId': 'user@red', 'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=3d4ea0bb-52ec-472a-8c00-b12f830c0d23', ''], 'id': '1715595297145', 'timestamp': DatetimeWithNanoseconds(2024, 5, 13, 10, 14, 57, 145000, tzinfo=datetime.timezone.utc), 'output': ''}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=3d4ea0bb-52ec-472a-8c00-b12f830c0d23\n"
     ]
    }
   ],
   "source": [
    "import models.content_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['stt_key1', 'stt_key2', 'openai_key1', 'openai_key2'])\n"
     ]
    }
   ],
   "source": [
    "# load azure keys\n",
    "import json\n",
    "with open('../keys/azure.json') as json_file:\n",
    "    azure_keys = json.load(json_file)\n",
    "print(azure_keys.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch files from url\n",
    "import requests\n",
    "\n",
    "def load_files_from_url(fileUrls):\n",
    "    urls=fileUrls\n",
    "    files=[]\n",
    "    for url in urls:\n",
    "        print(\"url------------------------\\n\",url)\n",
    "        local_filename = url.split('/')[-1].split(\"%2F\")[1].split(\"?\")[0]\n",
    "        local_filename='./data_dir/input_files/'+local_filename\n",
    "        # NOTE the stream=True parameter below\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(local_filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        file_path=local_filename\n",
    "        files.append(file_path)\n",
    "    print(\"recieved file paths---------------------------\\n\",files)\n",
    "    return files\n",
    "\n",
    "# load_files_from_url([' https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Ffiles%2Fmail1.txt?alt=media&token=840a21e2-a35c-481f-9ebb-677854630d02'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing text files\n",
    "def proc_txt_data(files):\n",
    "    txt_files=[f for f in files if '.txt' in f]\n",
    "    txt_data=\"\"\n",
    "    for tf in txt_files:\n",
    "        print(f\"reading {tf} file----------------\")\n",
    "        with open(tf) as f:\n",
    "            txt_data+='\\n'.join(f.readlines())\n",
    "    print(\"processed text data------------------------\\n\",txt_data)\n",
    "    return txt_data\n",
    "\n",
    "# txt_data=proc_txt_data(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from azure speech to text api : convert speech to text\n",
    "import sys\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def proc_video_data(files):\n",
    "    aud_files=[f for f in files if '.mp4' in f or '.wav' in f]\n",
    "    \n",
    "    aud_data=\"\"\n",
    "    for af in aud_files:\n",
    "        speech_key, service_region = azure_keys['stt_key1'], \"eastus\"\n",
    "        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "        audio_config = speechsdk.audio.AudioConfig(filename=af)\n",
    "        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "        result = speech_recognizer.recognize_once()\n",
    "        print(result.text)\n",
    "        aud_data+=\"\\n\"+result.text\n",
    "    return aud_data\n",
    "\n",
    "# aud_data=proc_video_data(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
    "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
    "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
    "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
    "\"\"\"\n",
    "\n",
    "# content=txt_data+aud_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary from azure open ai api\n",
    "def generate_summary(content):\n",
    "    \n",
    "    print(\"content to summarize----------------------\\n\",content.__str__())\n",
    "    prompt=f\"\"\" \n",
    "    Generate a summary of our team meeting, highlighting key action items, decisions made, and deadlines set in key points. \n",
    "    #meeting transcript start:\n",
    "    {content}\n",
    "    #meeting transcript end\n",
    "    \"\"\"\n",
    "    from openai import AzureOpenAI\n",
    "    # from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "    deployment = \"red-gpt\"\n",
    "    endpoint=\"https://genai-warriors-openai-test.openai.azure.com/\"\n",
    "    # token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        # azure_ad_token_provider=token_provider,\n",
    "        api_version=\"2024-02-01\",\n",
    "        api_key=azure_keys['openai_key1']\n",
    "    )\n",
    "    completion = client.completions.create(\n",
    "        model=deployment,\n",
    "        prompt=prompt,\n",
    "        temperature=0.3,\n",
    "        max_tokens=350,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    response=completion\n",
    "    print(response)\n",
    "    cfr=response.choices[0].content_filter_results\n",
    "    pfr=response.prompt_filter_results\n",
    "    res=response.choices[0].text\n",
    "\n",
    "\n",
    "    print(\"\\n\\nsummary--------------------------\\n\",res)\n",
    "    print(cfr,'\\n',pfr)\n",
    "    # return response\n",
    "    return res +'\\n\\nContent Filter results=\\n'+pfr.__str__()\n",
    "   \n",
    "\n",
    "# response=generate_summary(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listener function\n",
    "from models.content_model import ContentModel\n",
    "# %pip install firebase_admin\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "import datetime\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from pandas import Timestamp\n",
    "\n",
    "\n",
    "\n",
    "def listen_msgs():\n",
    "    coll_name='sdlc_cognizant'\n",
    "    user_uid='sdlc_cognizant_user'\n",
    "        \n",
    "    # Use a service account.\n",
    "    cred = credentials.Certificate(f'../keys/sa.json')\n",
    "    if not firebase_admin._apps:\n",
    "        app = firebase_admin.initialize_app(cred)\n",
    "\n",
    "    db = firestore.client()\n",
    "    \n",
    "    # Create an Event for notifying main thread.\n",
    "    callback_done = threading.Event()\n",
    "\n",
    "    # Create a callback on_snapshot function to capture changes\n",
    "    def on_snapshot(doc_snapshot, changes, read_time):\n",
    "        for doc in doc_snapshot:\n",
    "            print(f\"Received document snapshot: {doc.id}\")\n",
    "            print(doc.to_dict())\n",
    "            content=ContentModel.from_dict(doc.to_dict())\n",
    "            # complaint.complaintText=complaint.complaintText.toString()\n",
    "            # data=doc.to_dict()\n",
    "            # if content.audioURL!='':\n",
    "            #     content.complaintText=convert_stt(complaint=content)\n",
    "            # if content.lang!='english':\n",
    "            #     content.complaintText=translate(complaint=content)\n",
    "\n",
    "            files=load_files_from_url(content.fileUrls)\n",
    "            txt_data=proc_txt_data(files=files)\n",
    "            vid_data=proc_video_data(files=files)\n",
    "            summary=generate_summary(txt_data+vid_data)\n",
    "            # pred_res={'output':\"model response\"}\n",
    "\n",
    "            content.senderId='backend@red'\n",
    "            content.output=summary\n",
    "            # data['priority']=pred_res['output'].split(':')[-1]\n",
    "            content.ots=datetime.datetime.utcnow()\n",
    "            print(f'sending response: '+content.output)\n",
    "            # doc_id=str(round(time.time() * 1000))\n",
    "            doc_id=content.id\n",
    "            db.collection(coll_name).document(user_uid).collection('pContents').document(doc_id).set(content.__dict__)\n",
    "            print(f\"----------sent----------------with id: {doc_id} \")\n",
    "\n",
    "        callback_done.set()\n",
    "\n",
    "    doc_ref = db.collection(coll_name).document(user_uid).collection('rContents').document(\"content\")\n",
    "\n",
    "    # Watch the document\n",
    "    doc_watch = doc_ref.on_snapshot(on_snapshot)\n",
    "    print(\"listening for messages...\",)\n",
    "    \n",
    "    \n",
    "\n",
    "    while True:\n",
    "        print('', end='', flush=True)\n",
    "        time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listening for messages...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received document snapshot: content\n",
      "{'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=689fdea4-6c84-426c-b2fc-7a46ceceaec9'], 'senderId': 'user@red', 'ots': None, 'id': '1715595562413', 'timestamp': DatetimeWithNanoseconds(2024, 5, 13, 10, 19, 22, 413000, tzinfo=datetime.timezone.utc), 'output': ''}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=689fdea4-6c84-426c-b2fc-7a46ceceaec9\n",
      "recieved file paths---------------------------\n",
      " ['./data_dir/input_files/mail1.txt']\n",
      "reading ./data_dir/input_files/mail1.txt file----------------\n",
      "processed text data------------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "content to summarize----------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Completion(id='cmpl-9ON4IzVgVeLeoCXQ38jMv6Wa4gAo8', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=\"\\nKey Action Items:\\n1. Continue working on advancing AI techniques through a holistic, human-centric approach.\\n2. Develop and implement the XYZ-code as a joint representation for AI to better understand humans.\\n3. Focus on cross-domain transfer learning, spanning modalities and languages.\\n4. Train pre-trained models to jointly learn representations for a broad range of downstream AI tasks.\\n5. Ground the joint XYZ-code with external knowledge sources in the downstream AI tasks.\\n\\nDecisions Made:\\n1. The joint XYZ-code will be a foundational component in achieving a leap in AI capabilities.\\n2. The team will continue to focus on achieving human performance on benchmarks in various AI tasks.\\n3. The long-term vision is to have AI models that can speak, hear, see, and understand humans better.\\n\\nDeadlines Set:\\n1. The team will continue to work on advancing AI techniques and developing the XYZ-code.\\n2. The goal is to achieve human performance on benchmarks in various AI tasks within the next year.\\n3. The long-term vision of achieving multi-sensory and multilingual learning will be a continuous effort.\\n\\nKey Points:\\n1. The team is working on advancing AI techniques through a holistic, human-centric approach.\\n2. The XYZ-code, a joint representation of monolingual text, audio or visual sensory signals, and multilingual data, is a key component in achieving the team's long-term vision.\\n3. The team has achieved human performance on benchmarks in various AI tasks and will continue to work towards their more ambitious aspiration.\\n4. Grounding the joint XYZ-code with external knowledge sources will further enhance AI capabilities.\\n5. The team is committed to continuously improving and pushing the boundaries of AI.\", content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1715595574, model='gpt-35-turbo-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=340, prompt_tokens=349, total_tokens=689), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "\n",
      "\n",
      "summary--------------------------\n",
      " \n",
      "Key Action Items:\n",
      "1. Continue working on advancing AI techniques through a holistic, human-centric approach.\n",
      "2. Develop and implement the XYZ-code as a joint representation for AI to better understand humans.\n",
      "3. Focus on cross-domain transfer learning, spanning modalities and languages.\n",
      "4. Train pre-trained models to jointly learn representations for a broad range of downstream AI tasks.\n",
      "5. Ground the joint XYZ-code with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Decisions Made:\n",
      "1. The joint XYZ-code will be a foundational component in achieving a leap in AI capabilities.\n",
      "2. The team will continue to focus on achieving human performance on benchmarks in various AI tasks.\n",
      "3. The long-term vision is to have AI models that can speak, hear, see, and understand humans better.\n",
      "\n",
      "Deadlines Set:\n",
      "1. The team will continue to work on advancing AI techniques and developing the XYZ-code.\n",
      "2. The goal is to achieve human performance on benchmarks in various AI tasks within the next year.\n",
      "3. The long-term vision of achieving multi-sensory and multilingual learning will be a continuous effort.\n",
      "\n",
      "Key Points:\n",
      "1. The team is working on advancing AI techniques through a holistic, human-centric approach.\n",
      "2. The XYZ-code, a joint representation of monolingual text, audio or visual sensory signals, and multilingual data, is a key component in achieving the team's long-term vision.\n",
      "3. The team has achieved human performance on benchmarks in various AI tasks and will continue to work towards their more ambitious aspiration.\n",
      "4. Grounding the joint XYZ-code with external knowledge sources will further enhance AI capabilities.\n",
      "5. The team is committed to continuously improving and pushing the boundaries of AI.\n",
      "{'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}} \n",
      " [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "sending response: \n",
      "Key Action Items:\n",
      "1. Continue working on advancing AI techniques through a holistic, human-centric approach.\n",
      "2. Develop and implement the XYZ-code as a joint representation for AI to better understand humans.\n",
      "3. Focus on cross-domain transfer learning, spanning modalities and languages.\n",
      "4. Train pre-trained models to jointly learn representations for a broad range of downstream AI tasks.\n",
      "5. Ground the joint XYZ-code with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Decisions Made:\n",
      "1. The joint XYZ-code will be a foundational component in achieving a leap in AI capabilities.\n",
      "2. The team will continue to focus on achieving human performance on benchmarks in various AI tasks.\n",
      "3. The long-term vision is to have AI models that can speak, hear, see, and understand humans better.\n",
      "\n",
      "Deadlines Set:\n",
      "1. The team will continue to work on advancing AI techniques and developing the XYZ-code.\n",
      "2. The goal is to achieve human performance on benchmarks in various AI tasks within the next year.\n",
      "3. The long-term vision of achieving multi-sensory and multilingual learning will be a continuous effort.\n",
      "\n",
      "Key Points:\n",
      "1. The team is working on advancing AI techniques through a holistic, human-centric approach.\n",
      "2. The XYZ-code, a joint representation of monolingual text, audio or visual sensory signals, and multilingual data, is a key component in achieving the team's long-term vision.\n",
      "3. The team has achieved human performance on benchmarks in various AI tasks and will continue to work towards their more ambitious aspiration.\n",
      "4. Grounding the joint XYZ-code with external knowledge sources will further enhance AI capabilities.\n",
      "5. The team is committed to continuously improving and pushing the boundaries of AI.\n",
      "\n",
      "Content Filter results=\n",
      "[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "----------sent----------------with id: 1715595562413 \n",
      "Received document snapshot: content\n",
      "{'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=f09c2a75-4a34-4db5-9370-094fb7b45537'], 'senderId': 'user@red', 'ots': None, 'id': '1715595995706', 'output': '', 'timestamp': DatetimeWithNanoseconds(2024, 5, 13, 10, 26, 35, 706000, tzinfo=datetime.timezone.utc)}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=f09c2a75-4a34-4db5-9370-094fb7b45537\n",
      "recieved file paths---------------------------\n",
      " ['./data_dir/input_files/mail1.txt']\n",
      "reading ./data_dir/input_files/mail1.txt file----------------\n",
      "processed text data------------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "content to summarize----------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Completion(id='cmpl-9ONB9bpOXsTBVo1ms1iOKnJy00sbs', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text='\\nKey Action Items:\\n1. Develop a joint XYZ-code representation for AI learning and understanding.\\n2. Work towards achieving cross-domain transfer learning across modalities and languages.\\n3. Continue to improve and expand pre-trained models for downstream AI tasks.\\n4. Ground the joint XYZ-code with external knowledge sources.\\n\\nDecisions Made:\\n1. The joint XYZ-code will be a foundational component for achieving our long-term vision.\\n2. The team will focus on developing and utilizing the XYZ-code in all future AI projects.\\n\\nDeadlines:\\n1. Develop and implement the joint XYZ-code within the next 6 months.\\n2. Achieve cross-domain transfer learning within the next year.\\n3. Continuously improve and expand pre-trained models for downstream AI tasks on an ongoing basis.', content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1715595999, model='gpt-35-turbo-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=153, prompt_tokens=349, total_tokens=502), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "\n",
      "\n",
      "summary--------------------------\n",
      " \n",
      "Key Action Items:\n",
      "1. Develop a joint XYZ-code representation for AI learning and understanding.\n",
      "2. Work towards achieving cross-domain transfer learning across modalities and languages.\n",
      "3. Continue to improve and expand pre-trained models for downstream AI tasks.\n",
      "4. Ground the joint XYZ-code with external knowledge sources.\n",
      "\n",
      "Decisions Made:\n",
      "1. The joint XYZ-code will be a foundational component for achieving our long-term vision.\n",
      "2. The team will focus on developing and utilizing the XYZ-code in all future AI projects.\n",
      "\n",
      "Deadlines:\n",
      "1. Develop and implement the joint XYZ-code within the next 6 months.\n",
      "2. Achieve cross-domain transfer learning within the next year.\n",
      "3. Continuously improve and expand pre-trained models for downstream AI tasks on an ongoing basis.\n",
      "{'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}} \n",
      " [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "sending response: \n",
      "Key Action Items:\n",
      "1. Develop a joint XYZ-code representation for AI learning and understanding.\n",
      "2. Work towards achieving cross-domain transfer learning across modalities and languages.\n",
      "3. Continue to improve and expand pre-trained models for downstream AI tasks.\n",
      "4. Ground the joint XYZ-code with external knowledge sources.\n",
      "\n",
      "Decisions Made:\n",
      "1. The joint XYZ-code will be a foundational component for achieving our long-term vision.\n",
      "2. The team will focus on developing and utilizing the XYZ-code in all future AI projects.\n",
      "\n",
      "Deadlines:\n",
      "1. Develop and implement the joint XYZ-code within the next 6 months.\n",
      "2. Achieve cross-domain transfer learning within the next year.\n",
      "3. Continuously improve and expand pre-trained models for downstream AI tasks on an ongoing basis.\n",
      "\n",
      "Content Filter results=\n",
      "[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "----------sent----------------with id: 1715595995706 \n",
      "Received document snapshot: content\n",
      "{'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=55c341e2-21d3-4c0c-ada5-b611b5321feb'], 'senderId': 'user@red', 'ots': None, 'id': '1715596494396', 'output': '', 'timestamp': DatetimeWithNanoseconds(2024, 5, 13, 10, 34, 54, 396000, tzinfo=datetime.timezone.utc)}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=55c341e2-21d3-4c0c-ada5-b611b5321feb\n",
      "recieved file paths---------------------------\n",
      " ['./data_dir/input_files/mail1.txt']\n",
      "reading ./data_dir/input_files/mail1.txt file----------------\n",
      "processed text data------------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "content to summarize----------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Completion(id='cmpl-9ONJCppmVgFgeCHEaGDFRka90Nm2W', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=\"\\nKey Action Items:\\n1. Develop a joint XYZ-code representation for AI models to improve human-centric learning and understanding.\\n2. Continue working on cross-domain transfer learning to support a broad range of downstream AI tasks.\\n3. Incorporate external knowledge sources in the downstream AI tasks to enhance the joint XYZ-code representation.\\n4. Conduct further research and development to achieve multi-sensory and multilingual learning in AI.\\n\\nDecisions Made:\\n1. The team will focus on developing a joint XYZ-code representation for AI models.\\n2. The long-term vision is to achieve cross-domain transfer learning and multi-sensory, multilingual learning in AI.\\n3. The joint XYZ-code will be grounded with external knowledge sources in the downstream AI tasks.\\n\\nDeadlines:\\n1. Develop and implement the joint XYZ-code representation within the next 6 months.\\n2. Continue research and development to achieve cross-domain transfer learning and multi-sensory, multilingual learning in AI within the next 1 year.\\n\\nKey Points:\\n1. The team is working towards advancing AI beyond existing techniques by taking a human-centric approach.\\n2. The joint XYZ-code is a foundational component in achieving the team's long-term vision.\\n3. Five breakthroughs have been achieved in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\\n4. External knowledge sources will be incorporated to enhance the joint XYZ-code representation.\\n5. The team is aiming to achieve human-like learning and understanding in AI.\", content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1715596498, model='gpt-35-turbo-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=302, prompt_tokens=349, total_tokens=651), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "\n",
      "\n",
      "summary--------------------------\n",
      " \n",
      "Key Action Items:\n",
      "1. Develop a joint XYZ-code representation for AI models to improve human-centric learning and understanding.\n",
      "2. Continue working on cross-domain transfer learning to support a broad range of downstream AI tasks.\n",
      "3. Incorporate external knowledge sources in the downstream AI tasks to enhance the joint XYZ-code representation.\n",
      "4. Conduct further research and development to achieve multi-sensory and multilingual learning in AI.\n",
      "\n",
      "Decisions Made:\n",
      "1. The team will focus on developing a joint XYZ-code representation for AI models.\n",
      "2. The long-term vision is to achieve cross-domain transfer learning and multi-sensory, multilingual learning in AI.\n",
      "3. The joint XYZ-code will be grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Deadlines:\n",
      "1. Develop and implement the joint XYZ-code representation within the next 6 months.\n",
      "2. Continue research and development to achieve cross-domain transfer learning and multi-sensory, multilingual learning in AI within the next 1 year.\n",
      "\n",
      "Key Points:\n",
      "1. The team is working towards advancing AI beyond existing techniques by taking a human-centric approach.\n",
      "2. The joint XYZ-code is a foundational component in achieving the team's long-term vision.\n",
      "3. Five breakthroughs have been achieved in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n",
      "4. External knowledge sources will be incorporated to enhance the joint XYZ-code representation.\n",
      "5. The team is aiming to achieve human-like learning and understanding in AI.\n",
      "{'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}} \n",
      " [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "sending response: \n",
      "Key Action Items:\n",
      "1. Develop a joint XYZ-code representation for AI models to improve human-centric learning and understanding.\n",
      "2. Continue working on cross-domain transfer learning to support a broad range of downstream AI tasks.\n",
      "3. Incorporate external knowledge sources in the downstream AI tasks to enhance the joint XYZ-code representation.\n",
      "4. Conduct further research and development to achieve multi-sensory and multilingual learning in AI.\n",
      "\n",
      "Decisions Made:\n",
      "1. The team will focus on developing a joint XYZ-code representation for AI models.\n",
      "2. The long-term vision is to achieve cross-domain transfer learning and multi-sensory, multilingual learning in AI.\n",
      "3. The joint XYZ-code will be grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Deadlines:\n",
      "1. Develop and implement the joint XYZ-code representation within the next 6 months.\n",
      "2. Continue research and development to achieve cross-domain transfer learning and multi-sensory, multilingual learning in AI within the next 1 year.\n",
      "\n",
      "Key Points:\n",
      "1. The team is working towards advancing AI beyond existing techniques by taking a human-centric approach.\n",
      "2. The joint XYZ-code is a foundational component in achieving the team's long-term vision.\n",
      "3. Five breakthroughs have been achieved in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n",
      "4. External knowledge sources will be incorporated to enhance the joint XYZ-code representation.\n",
      "5. The team is aiming to achieve human-like learning and understanding in AI.\n",
      "\n",
      "Content Filter results=\n",
      "[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "----------sent----------------with id: 1715596494396 \n",
      "Received document snapshot: content\n",
      "{'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=b54f0f4b-782b-4010-bd9c-0ab277bcf351'], 'senderId': 'user@red', 'ots': None, 'id': '1715596802170', 'output': '', 'timestamp': DatetimeWithNanoseconds(2024, 5, 13, 10, 40, 2, 170000, tzinfo=datetime.timezone.utc)}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=b54f0f4b-782b-4010-bd9c-0ab277bcf351\n",
      "recieved file paths---------------------------\n",
      " ['./data_dir/input_files/mail1.txt']\n",
      "reading ./data_dir/input_files/mail1.txt file----------------\n",
      "processed text data------------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "content to summarize----------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Completion(id='cmpl-9ONO9g5h2pjyD9UF1fbjIFKwjS6Ns', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text='\\nKey Action Items:\\n1. Develop a joint XYZ-code to create more powerful AI that can speak, hear, see, and understand humans better.\\n2. Work towards achieving cross-domain transfer learning, spanning modalities and languages.\\n3. Train pre-trained models to jointly learn representations to support a broad range of downstream AI tasks.\\n4. Ground the joint XYZ-code with external knowledge sources in the downstream AI tasks.\\n\\nDecisions Made:\\n1. The joint XYZ-code will be a foundational component in achieving a leap in AI capabilities.\\n2. The team will focus on achieving multi-sensory and multilingual learning, closer to how humans learn and understand.\\n\\nDeadlines Set:\\n1. Develop the joint XYZ-code within the next 6 months.\\n2. Achieve cross-domain transfer learning within the next year.\\n3. Train pre-trained models within the next 2 years.\\n4. Ground the joint XYZ-code with external knowledge sources within the next 6 months.', content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1715596805, model='gpt-35-turbo-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=193, prompt_tokens=349, total_tokens=542), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "\n",
      "\n",
      "summary--------------------------\n",
      " \n",
      "Key Action Items:\n",
      "1. Develop a joint XYZ-code to create more powerful AI that can speak, hear, see, and understand humans better.\n",
      "2. Work towards achieving cross-domain transfer learning, spanning modalities and languages.\n",
      "3. Train pre-trained models to jointly learn representations to support a broad range of downstream AI tasks.\n",
      "4. Ground the joint XYZ-code with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Decisions Made:\n",
      "1. The joint XYZ-code will be a foundational component in achieving a leap in AI capabilities.\n",
      "2. The team will focus on achieving multi-sensory and multilingual learning, closer to how humans learn and understand.\n",
      "\n",
      "Deadlines Set:\n",
      "1. Develop the joint XYZ-code within the next 6 months.\n",
      "2. Achieve cross-domain transfer learning within the next year.\n",
      "3. Train pre-trained models within the next 2 years.\n",
      "4. Ground the joint XYZ-code with external knowledge sources within the next 6 months.\n",
      "{'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}} \n",
      " [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "sending response: \n",
      "Key Action Items:\n",
      "1. Develop a joint XYZ-code to create more powerful AI that can speak, hear, see, and understand humans better.\n",
      "2. Work towards achieving cross-domain transfer learning, spanning modalities and languages.\n",
      "3. Train pre-trained models to jointly learn representations to support a broad range of downstream AI tasks.\n",
      "4. Ground the joint XYZ-code with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Decisions Made:\n",
      "1. The joint XYZ-code will be a foundational component in achieving a leap in AI capabilities.\n",
      "2. The team will focus on achieving multi-sensory and multilingual learning, closer to how humans learn and understand.\n",
      "\n",
      "Deadlines Set:\n",
      "1. Develop the joint XYZ-code within the next 6 months.\n",
      "2. Achieve cross-domain transfer learning within the next year.\n",
      "3. Train pre-trained models within the next 2 years.\n",
      "4. Ground the joint XYZ-code with external knowledge sources within the next 6 months.\n",
      "\n",
      "Content Filter results=\n",
      "[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "----------sent----------------with id: 1715596802170 \n",
      "Received document snapshot: content\n",
      "{'ots': None, 'senderId': 'user@red', 'fileUrls': ['https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=1637cbcf-5536-4f05-a8a0-b8b91d0f7d6b'], 'id': '1715596929516', 'output': '', 'timestamp': DatetimeWithNanoseconds(2024, 5, 13, 10, 42, 9, 516000, tzinfo=datetime.timezone.utc)}\n",
      "url------------------------\n",
      " https://firebasestorage.googleapis.com/v0/b/mh-pred-app.appspot.com/o/sdlc_cognizant%2Fmail1.txt?alt=media&token=1637cbcf-5536-4f05-a8a0-b8b91d0f7d6b\n",
      "recieved file paths---------------------------\n",
      " ['./data_dir/input_files/mail1.txt']\n",
      "reading ./data_dir/input_files/mail1.txt file----------------\n",
      "processed text data------------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "content to summarize----------------------\n",
      " At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic,human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, \n",
      "\n",
      "I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition:\n",
      "\n",
      "monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three,there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak,\n",
      "\n",
      "hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Completion(id='cmpl-9ONQCXpLUiYANftkUzUVgBnWppYXe', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text='\\nKey Action Items:\\n1. Develop and implement the XYZ-code as a joint representation for AI learning and understanding.\\n2. Work towards achieving cross-domain transfer learning, spanning modalities and languages.\\n3. Continue to improve and expand pre-trained models to support a broad range of downstream AI tasks.\\n4. Ground the XYZ-code with external knowledge sources in the downstream AI tasks.\\n\\nDecisions Made:\\n1. The team will focus on developing and implementing the XYZ-code as a joint representation for AI learning and understanding.\\n2. The long-term vision is to achieve cross-domain transfer learning, spanning modalities and languages.\\n3. The team will continue to improve and expand pre-trained models to support a broad range of downstream AI tasks.\\n4. External knowledge sources will be used to ground the XYZ-code in the downstream AI tasks.\\n\\nDeadlines Set:\\n1. The team will aim to have the XYZ-code developed and implemented within the next six months.\\n2. The goal is to achieve cross-domain transfer learning within the next year.\\n3. Pre-trained models will be continuously improved and expanded over the next five years.\\n4. External knowledge sources will be incorporated into the XYZ-code within the next three months.', content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1715596932, model='gpt-35-turbo-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=239, prompt_tokens=349, total_tokens=588), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "\n",
      "\n",
      "summary--------------------------\n",
      " \n",
      "Key Action Items:\n",
      "1. Develop and implement the XYZ-code as a joint representation for AI learning and understanding.\n",
      "2. Work towards achieving cross-domain transfer learning, spanning modalities and languages.\n",
      "3. Continue to improve and expand pre-trained models to support a broad range of downstream AI tasks.\n",
      "4. Ground the XYZ-code with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Decisions Made:\n",
      "1. The team will focus on developing and implementing the XYZ-code as a joint representation for AI learning and understanding.\n",
      "2. The long-term vision is to achieve cross-domain transfer learning, spanning modalities and languages.\n",
      "3. The team will continue to improve and expand pre-trained models to support a broad range of downstream AI tasks.\n",
      "4. External knowledge sources will be used to ground the XYZ-code in the downstream AI tasks.\n",
      "\n",
      "Deadlines Set:\n",
      "1. The team will aim to have the XYZ-code developed and implemented within the next six months.\n",
      "2. The goal is to achieve cross-domain transfer learning within the next year.\n",
      "3. Pre-trained models will be continuously improved and expanded over the next five years.\n",
      "4. External knowledge sources will be incorporated into the XYZ-code within the next three months.\n",
      "{'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}} \n",
      " [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "sending response: \n",
      "Key Action Items:\n",
      "1. Develop and implement the XYZ-code as a joint representation for AI learning and understanding.\n",
      "2. Work towards achieving cross-domain transfer learning, spanning modalities and languages.\n",
      "3. Continue to improve and expand pre-trained models to support a broad range of downstream AI tasks.\n",
      "4. Ground the XYZ-code with external knowledge sources in the downstream AI tasks.\n",
      "\n",
      "Decisions Made:\n",
      "1. The team will focus on developing and implementing the XYZ-code as a joint representation for AI learning and understanding.\n",
      "2. The long-term vision is to achieve cross-domain transfer learning, spanning modalities and languages.\n",
      "3. The team will continue to improve and expand pre-trained models to support a broad range of downstream AI tasks.\n",
      "4. External knowledge sources will be used to ground the XYZ-code in the downstream AI tasks.\n",
      "\n",
      "Deadlines Set:\n",
      "1. The team will aim to have the XYZ-code developed and implemented within the next six months.\n",
      "2. The goal is to achieve cross-domain transfer learning within the next year.\n",
      "3. Pre-trained models will be continuously improved and expanded over the next five years.\n",
      "4. External knowledge sources will be incorporated into the XYZ-code within the next three months.\n",
      "\n",
      "Content Filter results=\n",
      "[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]\n",
      "----------sent----------------with id: 1715596929516 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# calling the listener function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mlisten_msgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror occured:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \n",
      "Cell \u001b[0;32mIn[86], line 71\u001b[0m, in \u001b[0;36mlisten_msgs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# calling the listener function\n",
    "try:\n",
    "    listen_msgs()\n",
    "except Exception as e:\n",
    "    print(f'error occured:\\n\\n\\n {e}')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
